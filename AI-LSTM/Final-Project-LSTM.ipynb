{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Stock Market Predictions with LSTM in Python</h1>\n",
    "    <h3 align=\"center\">Big Data Final Project</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"why-do-you-need-time-series-models?-youwo\">Why Do You Need Time Series Models?</h2>\n",
    "<p class=\"p-margin\">You would like to model stock prices correctly so as a stock buyer, you can reasonably decide when to buy stocks and when to sell them to make a profit. You need good machine learning models that can look at the history of a sequence of data and correctly predict what the future elements of the sequence are going to be.</p>\n",
    "<p class=\"p-margin\"><strong>Warning</strong>: Stock market prices are highly unpredictable and volatile. This means that no consistent patterns in the data allow you to model stock prices over time near-perfectly. Don't take it from me; take it from Princeton University economist Burton Malkiel, who argues in his 1973 book, \"A Random Walk Down Wall Street,\" that if the market is truly efficient and a share price reflects all factors immediately as soon as they're made public, a blindfolded monkey throwing darts at a newspaper stock listing should do as well as any investment professional.</p>\n",
    "<p class=\"p-margin\">However, let's not go all the way believing that this is just a stochastic or random process and that there is no hope for machine learning. Let's see if you can at least model the data so that the predictions you make correlate with the actual behavior of the data. In other words, you don't need the exact stock values of the future but the stock price movements (that is, if it is going to rise or fall in the near future).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you have all these libaries available to run the code successfully\n",
    "from pandas_datareader import data\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import urllib.request, json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_source = 'kaggle' # alphavantage or kaggle\n",
    "\n",
    "if data_source == 'alphavantage':\n",
    "    # ====================== Loading Data from Alpha Vantage ==================================\n",
    "\n",
    "    api_key = '<your API key>'\n",
    "\n",
    "    # American Airlines stock market prices\n",
    "    ticker = \"AAL\"\n",
    "\n",
    "    # JSON file with all the stock market data for AAL from the last 20 years\n",
    "    url_string = \"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol=%s&outputsize=full&apikey=%s\"%(ticker,api_key)\n",
    "\n",
    "    # Save data to this file\n",
    "    file_to_save = 'stock_market_data-%s.csv'%ticker\n",
    "\n",
    "    # If you haven't already saved data,\n",
    "    # Go ahead and grab the data from the url\n",
    "    # And store date, low, high, volume, close, open values to a Pandas DataFrame\n",
    "    if not os.path.exists(file_to_save):\n",
    "        with urllib.request.urlopen(url_string) as url:\n",
    "            data = json.loads(url.read().decode())\n",
    "            # extract stock market data\n",
    "            data = data['Time Series (Daily)']\n",
    "            df = pd.DataFrame(columns=['Date','Low','High','Close','Open'])\n",
    "            for k,v in data.items():\n",
    "                date = dt.datetime.strptime(k, '%Y-%m-%d')\n",
    "                data_row = [date.date(),float(v['3. low']),float(v['2. high']),\n",
    "                            float(v['4. close']),float(v['1. open'])]\n",
    "                df.loc[-1,:] = data_row\n",
    "                df.index = df.index + 1\n",
    "        print('Data saved to : %s'%file_to_save)        \n",
    "        df.to_csv(file_to_save)\n",
    "\n",
    "    # If the data is already there, just load it from the CSV\n",
    "    else:\n",
    "        print('File already exists. Loading data from CSV')\n",
    "        df = pd.read_csv(file_to_save)\n",
    "\n",
    "else:\n",
    "\n",
    "    # ====================== Loading Data from Kaggle ==================================\n",
    "    # You will be using HP's data. Feel free to experiment with other data.\n",
    "    # But while doing so, be careful to have a large enough dataset and also pay attention to the data normalization\n",
    "    df = pd.read_csv(os.path.join('Stocks','hpq.us.txt'),delimiter=',',usecols=['Date','Open','High','Low','Close'])\n",
    "    print('Loaded data from the Kaggle repository')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">Stock prices come in several different flavors. They are,</p>\n",
    "<ul>\n",
    "<li><strong>Open</strong>: Opening stock price of the day</li>\n",
    "<li><strong>Close</strong>: Closing stock price of the day</li>\n",
    "<li><strong>High</strong>: Highest stock price of the data</li>\n",
    "<li><strong>Low</strong>: Lowest stock price of the day</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"data-exploration-here,\">Data exploration</h3>\n",
    "<p class=\"p-margin\">Here, you will print the data you collected into the DataFrame. You should also make sure that the data is sorted by date because the order of the data is crucial in time series modeling.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by date\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Double check the result\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),(df['Low']+df['High'])/2.0)\n",
    "plt.xticks(range(0,df.shape[0],500),df['Date'].loc[::500],rotation=45)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data Into a Training Set and a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First calculate the mid prices from the highest and lowest\n",
    "high_prices = df.loc[:,'High'].as_matrix()\n",
    "low_prices = df.loc[:,'Low'].as_matrix()\n",
    "mid_prices = (high_prices+low_prices)/2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">Now, you can split the training data and test data. The training data will be the first 11,000 data points of the time series, and the rest will be test data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = mid_prices[:11000]\n",
    "test_data = mid_prices[11000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"normalizing-the-data-now,y\">Normalizing the Data</h2>\n",
    "<p class=\"p-margin\">Now, you need to define a scaler to normalize the data. <code>MinMaxScalar</code> scales all the data to be in the region of 0 and 1. You can also reshape the training and test data to be in the shape <code>[data_size, num_features]</code>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data to be between 0 and 1\n",
    "# When scaling remember! You normalize both test and train data with respect to training data\n",
    "# Because you are not supposed to have access to test data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data.reshape(-1,1)\n",
    "test_data = test_data.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">Due to your earlier observation that different time periods of data have different value ranges, you normalize the data by splitting the full series into windows. If you don't do this, the earlier data will be close to 0 and will not add much value to the learning process. Here, you choose a window size of 2500.</p>\n",
    "<p class=\"p-margin\"><strong>Tip</strong>: When choosing the window size, make sure it's not too small. When you perform windowed normalization, it can introduce a break at the very end of each window, as each window is normalized independently.</p>\n",
    "<p class=\"p-margin\">In this example, 4 data points will be affected by this. But given you have 11,000 data points, 4 points will not cause any issue.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the Scaler with training data and smooth data\n",
    "smoothing_window_size = 2500\n",
    "for di in range(0,10000,smoothing_window_size):\n",
    "    scaler.fit(train_data[di:di+smoothing_window_size,:])\n",
    "    train_data[di:di+smoothing_window_size,:] = scaler.transform(train_data[di:di+smoothing_window_size,:])\n",
    "\n",
    "# You normalize the last bit of remaining data\n",
    "scaler.fit(train_data[di+smoothing_window_size:,:])\n",
    "train_data[di+smoothing_window_size:,:] = scaler.transform(train_data[di+smoothing_window_size:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">Reshape the data back to the shape of <code>[data_size]</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape both train and test data\n",
    "train_data = train_data.reshape(-1)\n",
    "\n",
    "# Normalize test data\n",
    "test_data = scaler.transform(test_data).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">You can now smooth the data using the exponential moving average. This helps eliminate the inherent raggedness of stock prices and produce a smoother curve.</p>\n",
    "<p class=\"p-margin\"><strong>Note</strong> that you should only smooth training data.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now perform exponential moving average smoothing\n",
    "# So the data will have a smoother curve than the original ragged data\n",
    "EMA = 0.0\n",
    "gamma = 0.1\n",
    "for ti in range(11000):\n",
    "  EMA = gamma*train_data[ti] + (1-gamma)*EMA\n",
    "  train_data[ti] = EMA\n",
    "\n",
    "# Used for visualization and test purposes\n",
    "all_mid_data = np.concatenate([train_data,test_data],axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Step Ahead Prediction via Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">Averaging mechanisms allow you to predict (often a one-time step ahead) by representing the future stock price as an average of the previously observed stock prices. Doing this for more than one time step can produce quite bad results. You will look at two averaging techniques: below standard averaging and exponential moving average. You will evaluate both qualitatively (visual inspection) and quantitatively (Mean Squared Error) the results produced by the two algorithms.</p>\n",
    "<p class=\"p-margin\">The Mean Squared Error (MSE) can be calculated by taking the Squared Error between the true value at one step ahead and the predicted value and averaging it over all the predictions.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">You can understand the difficulty of this problem by first trying to model this as an average calculation problem. First, you will try to predict the future stock market prices (for example, x<sub>t+1</sub> ) as an average of the previously observed stock market prices within a fixed size window (for example, x<sub>t-N</sub>, ..., x<sub>t</sub>) (say previous 100 days). Thereafter, you will try a bit more fancier \"exponential moving average\" method and see how well that does. Then, you will move on to the \"holy grail\" of time-series prediction: Long Short-Term Memory models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "std_avg_predictions = []\n",
    "std_avg_x = []\n",
    "mse_errors = []\n",
    "\n",
    "for pred_idx in range(window_size,N):\n",
    "\n",
    "    if pred_idx >= N:\n",
    "        date = dt.datetime.strptime(k, '%Y-%m-%d').date() + dt.timedelta(days=1)\n",
    "    else:\n",
    "        date = df.loc[pred_idx,'Date']\n",
    "\n",
    "    std_avg_predictions.append(np.mean(train_data[pred_idx-window_size:pred_idx]))\n",
    "    mse_errors.append((std_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    std_avg_x.append(date)\n",
    "\n",
    "print('MSE error for standard averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(window_size,N),std_avg_predictions,color='orange',label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 100\n",
    "N = train_data.size\n",
    "\n",
    "run_avg_predictions = []\n",
    "run_avg_x = []\n",
    "\n",
    "mse_errors = []\n",
    "\n",
    "running_mean = 0.0\n",
    "run_avg_predictions.append(running_mean)\n",
    "\n",
    "decay = 0.5\n",
    "\n",
    "for pred_idx in range(1,N):\n",
    "\n",
    "    running_mean = running_mean*decay + (1.0-decay)*train_data[pred_idx-1]\n",
    "    run_avg_predictions.append(running_mean)\n",
    "    mse_errors.append((run_avg_predictions[-1]-train_data[pred_idx])**2)\n",
    "    run_avg_x.append(date)\n",
    "\n",
    "print('MSE error for EMA averaging: %.5f'%(0.5*np.mean(mse_errors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (18,9))\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b',label='True')\n",
    "plt.plot(range(0,N),run_avg_predictions,color='orange', label='Prediction')\n",
    "#plt.xticks(range(0,df.shape[0],50),df['Date'].loc[::50],rotation=45)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mid Price')\n",
    "plt.legend(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the exponential moving average is this good, why do you need better models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\">You see that it fits a perfect line that follows the <code>True</code> distribution (and justified by the very low MSE). Practically speaking, you can't do much with just the stock market value of the next day.&nbsp;<span style=\"box-sizing: border-box;\">Personally, what I'd like is not the exact stock market price for the next day, but&nbsp;<em>would the stock market prices go up or down in the next 30 days?</em></span>&nbsp;Try to do this, and you will expose the incapability of the EMA method.</p>\n",
    "<p class=\"p-margin\">You will now try to make predictions in windows (say you predict the next 2 days window, instead of just the next day). Then, you will realize how wrong EMA can go. Here is an example:</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"predict-more-than-one-step-into-the-future-tomak\">Predict more than one step into the future</h3>\n",
    "<p class=\"p-margin\">To make things concrete, let's assume values, say $x_t=0.4$, $EMA=0.5$ and $\\gamma = 0.5$</p>\n",
    "<ul>\n",
    "<li>Say you get the output with the following equation\n",
    "<ul>\n",
    "<li>X<sub>t+1</sub> = EMA<sub>t</sub> = γ × EMA<sub>t-1</sub> + (1 - γ)X<sub>t</sub></li>\n",
    "<li>So you have x<sub>t+1</sub> = 0.5 × 0.5 + (1-0.5) × 0.4 = 0.45</li>\n",
    "<li>So X<sub>t+1</sub> = EMA<sub>t</sub> = 0.45</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>So the next prediction X<sub>t+2</sub> becomes,\n",
    "<ul>\n",
    "<li>X<sub>t+2</sub> = γ × EMA<sub>t</sub> + (1-γ)X<sub>t+1</sub></li>\n",
    "<li>Which is X<sub>t+2</sub> = γ × EMA_t + (1-γ) EMA<sub>t</sub> = EMA<sub>t</sub></li>\n",
    "<li>Or in this example, X<sub>t+2</sub> = X<sub>t+1</sub> = 0.45</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "<p class=\"p-margin\">So, no matter how many steps you predict in the future, you'll keep getting the same answer for all the future prediction steps.</p>\n",
    "<p class=\"p-margin\">One solution you have that will output useful information is to look at <strong>momentum-based algorithms</strong>. They make predictions based on whether the past recent values were going up or going down (not the exact values). For example, they will say the next-day price will likely be lower if the prices have been dropping for the past few days, which sounds reasonable. However, you will use a more complex model: an LSTM model.</p>\n",
    "<p class=\"p-margin\">These models have taken the realm of time series prediction by storm because they are so good at modeling time series data. You will see if there actually are patterns hidden in the data that you can exploit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"introduction-to-lstms:-making-stock-movement-predictions-far-into-the-future-long-\">Introduction to LSTMs: Making Stock Movement Predictions Far into the Future</h2>\n",
    "<p class=\"p-margin\">Long-short-term memory models are extremely powerful time-series models. They can predict an arbitrary number of steps into the future. An LSTM module (or cell) has 5 essential components, which allow it to model both long-term and short-term data.</p>\n",
    "<ul>\n",
    "<li><strong>Cell state (c<sub>t</sub>)</strong> - This represents the internal memory of the cell, which stores both short-term memory and long-term memories.</li>\n",
    "<li><strong>Hidden state (ht)&nbsp;</strong>- This is output state information calculated w.r.t. current input, previous hidden state, and current cell input, which you eventually use to predict the future stock market prices. Additionally, the hidden state can decide to only retrieve short- or long-term memories or both types of memory stored in the cell state to make the next prediction.</li>\n",
    "<li><strong>Input gate (i<sub>t</sub>)</strong> - Decides how much information from current input flows to the cell state.</li>\n",
    "<li><strong>Forget gate (f<sub>t</sub>)</strong> - Decides how much information from the current input and the previous cell state flows into the current cell state.</li>\n",
    "<li><strong>Output gate (o<sub>t</sub>) </strong>- Decides how much information from the current cell state flows into the hidden state so that if needed LSTM can only pick the long-term memories or short-term memories and long-term memories.</li>\n",
    "</ul>\n",
    "<p class=\"p-margin\">A cell is pictured below:</p>\n",
    "<p class=\"p-margin\"><img height=\"447\" width=\"465\" src=\"https://media.datacamp.com/cms/ad_4nxfa5o5u1xdcegg7_p2ykgbb_j7ix4jnf3qd9bgeulqjw3ckz2uolgz0uv6ya_b2fyvrqzmrnsyhpzlyb1ijgzdmku637qdv3dzlnmkpt3i8hii4aynjtbisid2riifgmya6st_o6a.png\" loading=\"lazy\"></p>\n",
    "<p class=\"p-margin\">The equations for calculating each of these entities are as follows.</p>\n",
    "<ul>\n",
    "<li>i<sub>t</sub> = σ(W<sub>ix</sub>X<sub>t</sub> + W<sub>ih</sub>h<sub>t-1</sub> + b<sub>i</sub>)</li>\n",
    "<li>$\\tilde{c}\\_t = \\sigma(W\\_{cx}x\\_t + W\\_{ch}h_{t-1} + b_c)$</li>\n",
    "<li>$f\\_t = \\sigma(W\\_{fx}x\\_t + W\\_{fh}h_{t-1}+b_f)$</li>\n",
    "<li>$c_t = f\\_t c\\_{t-1} + i_t \\tilde{c}_t$</li>\n",
    "<li>$o\\_t = \\sigma(W\\_{ox}x\\_t + W\\_{oh}h_{t-1}+b_o)$</li>\n",
    "<li>$h_t = o_t tanh(c_t)$</li>\n",
    "</ul>\n",
    "<p class=\"p-margin\"><span style=\"box-sizing: border-box;\">You can refer to this article for a better (more technical) <a href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\" target=\"_blank\" rel=\"noopener\">understanding of LSTMs</a>.</span></p>\n",
    "<p class=\"p-margin\">TensorFlow provides a nice API (called RNN API) for implementing time series models. You will be using that for your implementations.</p>\n",
    "<h3 id=\"data-generator-youar\">Data generator</h3>\n",
    "<p class=\"p-margin\">You are first going to implement a data generator to train your model. This data generator will have a method called <code>.unroll_batches(...)</code> which will output a set of <em>num_unrollings</em> batches of input data obtained sequentially, where a batch of data is of size <code>[batch_size, 1]</code>. Then, each batch of input data will have a corresponding output batch of data.</p>\n",
    "<p class=\"p-margin\">For example if <code>num_unrollings=3</code> and <code>batch_size=4</code> a set of unrolled batches it might look like,</p>\n",
    "<ul>\n",
    "<li>input data: $[x_0,x_10,x_20,x_30], [x_1,x_11,x_21,x_31], [x_2,x_12,x_22,x_32]$</li>\n",
    "<li>output data: $[x_1,x_11,x_21,x_31], [x_2,x_12,x_22,x_32], [x_3,x_13,x_23,x_33]$</li>\n",
    "</ul>\n",
    "<h4 id=\"data-augmentation-also,\">Data augmentation</h4>\n",
    "<p class=\"p-margin\">Also, to make your model robust, you will not make the output for $x\\_t$ always $x\\_{t+1}$. Rather you will randomly sample an output from the set $x\\_{t+1},x\\_{t+2},\\ldots,x_{t+N}$ where $N$ is a small window size.</p>\n",
    "<p class=\"p-margin\">Here, you are making the following assumption:</p>\n",
    "<ul>\n",
    "<li>$x\\_{t+1},x\\_{t+2},\\ldots,x_{t+N}$ will not be very far from each other</li>\n",
    "</ul>\n",
    "<p class=\"p-margin\">I personally think this is a reasonable assumption for stock movement predictions.</p>\n",
    "<p class=\"p-margin\">Below, you illustrate how a batch of data is created visually.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p class=\"p-margin\"><img height=\"322\" width=\"529\" src=\"https://media.datacamp.com/cms/ad_4nxdog0hincolh_rp3ln7r-yu0w2sjmrffkfhrru5lzgkcxqd3ubzyfdevgjat2xuauusdujxb752cv8l6tayzwu8xhnf_9vkypc2hhr3oiepi4trwokvupij7puiaswnfgvsqaml.png\" loading=\"lazy\"></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorSeq(object):\n",
    "\n",
    "    def __init__(self,prices,batch_size,num_unroll):\n",
    "        self._prices = prices\n",
    "        self._prices_length = len(self._prices) - num_unroll\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unroll = num_unroll\n",
    "        self._segments = self._prices_length //self._batch_size\n",
    "        self._cursor = [offset * self._segments for offset in range(self._batch_size)]\n",
    "\n",
    "    def next_batch(self):\n",
    "\n",
    "        batch_data = np.zeros((self._batch_size),dtype=np.float32)\n",
    "        batch_labels = np.zeros((self._batch_size),dtype=np.float32)\n",
    "\n",
    "        for b in range(self._batch_size):\n",
    "            if self._cursor[b]+1>=self._prices_length:\n",
    "                #self._cursor[b] = b * self._segments\n",
    "                self._cursor[b] = np.random.randint(0,(b+1)*self._segments)\n",
    "\n",
    "            batch_data[b] = self._prices[self._cursor[b]]\n",
    "            batch_labels[b]= self._prices[self._cursor[b]+np.random.randint(0,5)]\n",
    "\n",
    "            self._cursor[b] = (self._cursor[b]+1)%self._prices_length\n",
    "\n",
    "        return batch_data,batch_labels\n",
    "\n",
    "    def unroll_batches(self):\n",
    "\n",
    "        unroll_data,unroll_labels = [],[]\n",
    "        init_data, init_label = None,None\n",
    "        for ui in range(self._num_unroll):\n",
    "\n",
    "            data, labels = self.next_batch()    \n",
    "\n",
    "            unroll_data.append(data)\n",
    "            unroll_labels.append(labels)\n",
    "\n",
    "        return unroll_data, unroll_labels\n",
    "\n",
    "    def reset_indices(self):\n",
    "        for b in range(self._batch_size):\n",
    "            self._cursor[b] = np.random.randint(0,min((b+1)*self._segments,self._prices_length-1))\n",
    "\n",
    "\n",
    "\n",
    "dg = DataGeneratorSeq(train_data,5,5)\n",
    "u_data, u_labels = dg.unroll_batches()\n",
    "\n",
    "for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):   \n",
    "    print('\\n\\nUnrolled index %d'%ui)\n",
    "    dat_ind = dat\n",
    "    lbl_ind = lbl\n",
    "    print('\\tInputs: ',dat )\n",
    "    print('\\n\\tOutput:',lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"defining-hyperparameters-inthi\">Defining hyperparameters</h3>\n",
    "<p class=\"p-margin\">In this section, you'll define several hyperparameters. <code>D</code> is the dimensionality of the input. It's straightforward, as you take the previous stock price as the input and predict the next one, which should be <code>1</code>.</p>\n",
    "<p class=\"p-margin\">Then you have <code>num_unrollings</code>, this is a hyperparameter related to the backpropagation through time (BPTT) that is used to optimize the LSTM model. This denotes how many continuous time steps you consider for a single optimization step. You can think of this as, instead of optimizing the model by looking at a single time step, you optimize the network by looking at <code>num_unrollings</code> time steps. The larger the better.</p>\n",
    "<p class=\"p-margin\">Then you have the <code>batch_size</code>. Batch size is how many data samples you consider in a single time step.</p>\n",
    "<p class=\"p-margin\">Next you define <code>num_nodes</code> which represents the number of hidden neurons in each cell. You can see that there are three layers of LSTMs in this example.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 1 # Dimensionality of the data. Since your data is 1-D this would be 1\n",
    "num_unrollings = 50 # Number of time steps you look into the future.\n",
    "batch_size = 500 # Number of samples in a batch\n",
    "num_nodes = [200,200,150] # Number of hidden nodes in each layer of the deep LSTM stack we're using\n",
    "n_layers = len(num_nodes) # number of layers\n",
    "dropout = 0.2 # dropout amount\n",
    "\n",
    "tf.reset_default_graph() # This is important in case you run this multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"defining-inputs-and-outputs-next,\">Defining inputs and outputs</h3>\n",
    "<p class=\"p-margin\">Next, you define placeholders for training inputs and labels. This is very straightforward as you have a list of input placeholders, each containing a single batch of data. And the list has <code>num_unrollings</code> placeholders, that will be used at once for a single optimization step.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_inputs, train_outputs = [],[]\n",
    "\n",
    "# You unroll the input over time defining placeholders for each time step\n",
    "for ui in range(num_unrollings):\n",
    "    train_inputs.append(tf.placeholder(tf.float32, shape=[batch_size,D],name='train_inputs_%d'%ui))\n",
    "    train_outputs.append(tf.placeholder(tf.float32, shape=[batch_size,1], name = 'train_outputs_%d'%ui))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"defining-parameters-of-the-lstm-and-regression-layer-youwi\">Defining parameters of the LSTM and regression layer</h3>\n",
    "<p class=\"p-margin\">You will have a three layers of LSTMs and a linear regression layer, denoted by <code>w</code> and <code>b</code>, that takes the output of the last Long Short-Term Memory cell and output the prediction for the next time step. You can use the <code>MultiRNNCell</code> in TensorFlow to encapsulate the three <code>LSTMCell</code> objects you created. Additionally, you can have the dropout implemented LSTM cells, as they improve performance and reduce overfitting.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cells = [\n",
    "    tf.contrib.rnn.LSTMCell(num_units=num_nodes[li],\n",
    "                            state_is_tuple=True,\n",
    "                            initializer= tf.contrib.layers.xavier_initializer()\n",
    "                           )\n",
    " for li in range(n_layers)]\n",
    "\n",
    "drop_lstm_cells = [tf.contrib.rnn.DropoutWrapper(\n",
    "    lstm, input_keep_prob=1.0,output_keep_prob=1.0-dropout, state_keep_prob=1.0-dropout\n",
    ") for lstm in lstm_cells]\n",
    "drop_multi_cell = tf.contrib.rnn.MultiRNNCell(drop_lstm_cells)\n",
    "multi_cell = tf.contrib.rnn.MultiRNNCell(lstm_cells)\n",
    "\n",
    "w = tf.get_variable('w',shape=[num_nodes[-1], 1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable('b',initializer=tf.random_uniform([1],-0.1,0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"calculating-lstm-output-and-feeding-it-to-the-regression-layer-to-get-the-final-prediction-inthi\">Calculating LSTM output and feeding it to the regression layer to get the final prediction</h3>\n",
    "<p class=\"p-margin\">In this section, you first create TensorFlow variables (<code>c</code> and <code>h</code>) that will hold the cell state and the hidden state of the Long Short-Term Memory cell. Then you transform the list of <code>train_inputs</code> to have a shape of <code>[num_unrollings, batch_size, D]</code>, this is needed for calculating the outputs with the <code>tf.nn.dynamic_rnn</code> function. You then calculate the LSTM outputs with the <code>tf.nn.dynamic_rnn</code> function and split the output back to a list of <code>num_unrolling</code> tensors. the loss between the predictions and true stock prices.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cell state and hidden state variables to maintain the state of the LSTM\n",
    "c, h = [],[]\n",
    "initial_state = []\n",
    "for li in range(n_layers):\n",
    "  c.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  h.append(tf.Variable(tf.zeros([batch_size, num_nodes[li]]), trainable=False))\n",
    "  initial_state.append(tf.contrib.rnn.LSTMStateTuple(c[li], h[li]))\n",
    "\n",
    "# Do several tensor transofmations, because the function dynamic_rnn requires the output to be of\n",
    "# a specific format. Read more at: https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "all_inputs = tf.concat([tf.expand_dims(t,0) for t in train_inputs],axis=0)\n",
    "\n",
    "# all_outputs is [seq_length, batch_size, num_nodes]\n",
    "all_lstm_outputs, state = tf.nn.dynamic_rnn(\n",
    "    drop_multi_cell, all_inputs, initial_state=tuple(initial_state),\n",
    "    time_major = True, dtype=tf.float32)\n",
    "\n",
    "all_lstm_outputs = tf.reshape(all_lstm_outputs, [batch_size*num_unrollings,num_nodes[-1]])\n",
    "\n",
    "all_outputs = tf.nn.xw_plus_b(all_lstm_outputs,w,b)\n",
    "\n",
    "split_outputs = tf.split(all_outputs,num_unrollings,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"loss-calculation-and-optimizer-now,y\">Loss calculation and optimizer</h3>\n",
    "<p class=\"p-margin\">Now, you'll calculate the loss. However, you should note that there is a unique characteristic when calculating the loss. For each batch of predictions and true outputs, you calculate the Mean Squared Error. And you sum (not average) all these mean squared losses together. Finally, you define the optimizer you're going to use to optimize the neural network. In this case, you can use Adam, which is a very recent and well-performing optimizer.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calculating the loss you need to be careful about the exact form, because you calculate\n",
    "# loss of all the unrolled steps at the same time\n",
    "# Therefore, take the mean error or each batch and get the sum of that over all the unrolled steps\n",
    "\n",
    "print('Defining training Loss')\n",
    "loss = 0.0\n",
    "with tf.control_dependencies([tf.assign(c[li], state[li][0]) for li in range(n_layers)]+\n",
    "                             [tf.assign(h[li], state[li][1]) for li in range(n_layers)]):\n",
    "  for ui in range(num_unrollings):\n",
    "    loss += tf.reduce_mean(0.5*(split_outputs[ui]-train_outputs[ui])**2)\n",
    "\n",
    "print('Learning rate decay operations')\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "inc_gstep = tf.assign(global_step,global_step + 1)\n",
    "tf_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "tf_min_learning_rate = tf.placeholder(shape=None,dtype=tf.float32)\n",
    "\n",
    "learning_rate = tf.maximum(\n",
    "    tf.train.exponential_decay(tf_learning_rate, global_step, decay_steps=1, decay_rate=0.5, staircase=True),\n",
    "    tf_min_learning_rate)\n",
    "\n",
    "# Optimizer.\n",
    "print('TF Optimization operations')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v))\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"prediction-related-calculations-herey\">Prediction related calculations</h3>\n",
    "<p class=\"p-margin\">Here you define the prediction related TensorFlow operations. First, define a placeholder for feeding in the input (<code>sample_inputs</code>), then similar to the training stage, you define state variables for prediction (<code>sample_c</code> and <code>sample_h</code>). Finally you calculate the prediction with the <code>tf.nn.dynamic_rnn</code> function and then sending the output through the regression layer (<code>w</code> and <code>b</code>). You also should define the <code>reset_sample_state</code> operation, which resets the cell state and the hidden state. You should execute this operation at the start, every time you make a sequence of predictions.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Defining prediction related TF functions')\n",
    "\n",
    "sample_inputs = tf.placeholder(tf.float32, shape=[1,D])\n",
    "\n",
    "# Maintaining LSTM state for prediction stage\n",
    "sample_c, sample_h, initial_sample_state = [],[],[]\n",
    "for li in range(n_layers):\n",
    "  sample_c.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  sample_h.append(tf.Variable(tf.zeros([1, num_nodes[li]]), trainable=False))\n",
    "  initial_sample_state.append(tf.contrib.rnn.LSTMStateTuple(sample_c[li],sample_h[li]))\n",
    "\n",
    "reset_sample_states = tf.group(*[tf.assign(sample_c[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)],\n",
    "                               *[tf.assign(sample_h[li],tf.zeros([1, num_nodes[li]])) for li in range(n_layers)])\n",
    "\n",
    "sample_outputs, sample_state = tf.nn.dynamic_rnn(multi_cell, tf.expand_dims(sample_inputs,0),\n",
    "                                   initial_state=tuple(initial_sample_state),\n",
    "                                   time_major = True,\n",
    "                                   dtype=tf.float32)\n",
    "\n",
    "with tf.control_dependencies([tf.assign(sample_c[li],sample_state[li][0]) for li in range(n_layers)]+\n",
    "                              [tf.assign(sample_h[li],sample_state[li][1]) for li in range(n_layers)]):  \n",
    "  sample_prediction = tf.nn.xw_plus_b(tf.reshape(sample_outputs,[1,-1]), w, b)\n",
    "\n",
    "print('\\tAll done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"running-the-lstm-herey\">Running the LSTM</h3>\n",
    "<p class=\"p-margin\">Here you will train and predict stock price movements for several epochs and see whether the predictions get better or worse over time. You follow the following procedure.</p>\n",
    "<ul>\n",
    "<li>Define a test set of starting points (<code>test_points_seq</code>) on the time series to evaluate the model on</li>\n",
    "<li>For each epoch\n",
    "<ul>\n",
    "<li>For full sequence length of training data\n",
    "<ul>\n",
    "<li>Unroll a set of <code>num_unrollings</code> batches</li>\n",
    "<li>Train the neural network with the unrolled batches</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>Calculate the average training loss</li>\n",
    "<li>For each starting point in the test set\n",
    "<ul>\n",
    "<li>Update the LSTM state by iterating through the previous <code>num_unrollings</code> data points found before the test point</li>\n",
    "<li>Make predictions for <code>n_predict_once</code> steps continuously, using the previous prediction as the current input</li>\n",
    "<li>Calculate the MSE loss between the <code>n_predict_once</code> points predicted and the true stock prices at those time stamps</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "valid_summary = 1 # Interval you make test predictions\n",
    "\n",
    "n_predict_once = 50 # Number of steps you continously predict for\n",
    "\n",
    "train_seq_length = train_data.size # Full length of the training data\n",
    "\n",
    "train_mse_ot = [] # Accumulate Train losses\n",
    "test_mse_ot = [] # Accumulate Test loss\n",
    "predictions_over_time = [] # Accumulate predictions\n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# Used for decaying learning rate\n",
    "loss_nondecrease_count = 0\n",
    "loss_nondecrease_threshold = 2 # If the test error hasn't increased in this many steps, decrease learning rate\n",
    "\n",
    "print('Initialized')\n",
    "average_loss = 0\n",
    "\n",
    "# Define data generator\n",
    "data_gen = DataGeneratorSeq(train_data,batch_size,num_unrollings)\n",
    "\n",
    "x_axis_seq = []\n",
    "\n",
    "# Points you start your test predictions from\n",
    "test_points_seq = np.arange(11000,12000,50).tolist()\n",
    "\n",
    "for ep in range(epochs):       \n",
    "\n",
    "    # ========================= Training =====================================\n",
    "    for step in range(train_seq_length//batch_size):\n",
    "\n",
    "        u_data, u_labels = data_gen.unroll_batches()\n",
    "\n",
    "        feed_dict = {}\n",
    "        for ui,(dat,lbl) in enumerate(zip(u_data,u_labels)):            \n",
    "            feed_dict[train_inputs[ui]] = dat.reshape(-1,1)\n",
    "            feed_dict[train_outputs[ui]] = lbl.reshape(-1,1)\n",
    "\n",
    "        feed_dict.update({tf_learning_rate: 0.0001, tf_min_learning_rate:0.000001})\n",
    "\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "\n",
    "        average_loss += l\n",
    "\n",
    "    # ============================ Validation ==============================\n",
    "    if (ep+1) % valid_summary == 0:\n",
    "\n",
    "      average_loss = average_loss/(valid_summary*(train_seq_length//batch_size))\n",
    "\n",
    "      # The average loss\n",
    "      if (ep+1)%valid_summary==0:\n",
    "        print('Average loss at step %d: %f' % (ep+1, average_loss))\n",
    "\n",
    "      train_mse_ot.append(average_loss)\n",
    "\n",
    "      average_loss = 0 # reset loss\n",
    "\n",
    "      predictions_seq = []\n",
    "\n",
    "      mse_test_loss_seq = []\n",
    "\n",
    "      # ===================== Updating State and Making Predicitons ========================\n",
    "      for w_i in test_points_seq:\n",
    "        mse_test_loss = 0.0\n",
    "        our_predictions = []\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          # Only calculate x_axis values in the first validation epoch\n",
    "          x_axis=[]\n",
    "\n",
    "        # Feed in the recent past behavior of stock prices\n",
    "        # to make predictions from that point onwards\n",
    "        for tr_i in range(w_i-num_unrollings+1,w_i-1):\n",
    "          current_price = all_mid_data[tr_i]\n",
    "          feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)    \n",
    "          _ = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "        feed_dict = {}\n",
    "\n",
    "        current_price = all_mid_data[w_i-1]\n",
    "\n",
    "        feed_dict[sample_inputs] = np.array(current_price).reshape(1,1)\n",
    "\n",
    "        # Make predictions for this many steps\n",
    "        # Each prediction uses previous prediciton as it's current input\n",
    "        for pred_i in range(n_predict_once):\n",
    "\n",
    "          pred = session.run(sample_prediction,feed_dict=feed_dict)\n",
    "\n",
    "          our_predictions.append(np.asscalar(pred))\n",
    "\n",
    "          feed_dict[sample_inputs] = np.asarray(pred).reshape(-1,1)\n",
    "\n",
    "          if (ep+1)-valid_summary==0:\n",
    "            # Only calculate x_axis values in the first validation epoch\n",
    "            x_axis.append(w_i+pred_i)\n",
    "\n",
    "          mse_test_loss += 0.5*(pred-all_mid_data[w_i+pred_i])**2\n",
    "\n",
    "        session.run(reset_sample_states)\n",
    "\n",
    "        predictions_seq.append(np.array(our_predictions))\n",
    "\n",
    "        mse_test_loss /= n_predict_once\n",
    "        mse_test_loss_seq.append(mse_test_loss)\n",
    "\n",
    "        if (ep+1)-valid_summary==0:\n",
    "          x_axis_seq.append(x_axis)\n",
    "\n",
    "      current_test_mse = np.mean(mse_test_loss_seq)\n",
    "\n",
    "      # Learning rate decay logic\n",
    "      if len(test_mse_ot)>0 and current_test_mse > min(test_mse_ot):\n",
    "          loss_nondecrease_count += 1\n",
    "      else:\n",
    "          loss_nondecrease_count = 0\n",
    "\n",
    "      if loss_nondecrease_count > loss_nondecrease_threshold :\n",
    "            session.run(inc_gstep)\n",
    "            loss_nondecrease_count = 0\n",
    "            print('\\tDecreasing learning rate by 0.5')\n",
    "\n",
    "      test_mse_ot.append(current_test_mse)\n",
    "      print('\\tTest MSE: %.5f'%np.mean(mse_test_loss_seq))\n",
    "      predictions_over_time.append(predictions_seq)\n",
    "      print('\\tFinished Predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"visualizing-the-predictions-youca\">Visualizing the Predictions</h2>\n",
    "<p class=\"p-margin\">You can see how the MSE loss is going down with the amount of training. This is a good sign that the model is learning something useful. To quantify your findings, you can compare the network's MSE loss to the MSE loss you obtained when doing the standard averaging (0.004). You can see that the LSTM is doing better than the standard averaging. And you know that standard averaging (though not perfect) followed the true stock prices movements reasonably.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_prediction_epoch = 28 # replace this with the epoch that you got the best results when running the plotting code\n",
    "\n",
    "plt.figure(figsize = (18,18))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "\n",
    "# Plotting how the predictions change over time\n",
    "# Plot older predictions with low alpha and newer predictions with high alpha\n",
    "start_alpha = 0.25\n",
    "alpha  = np.arange(start_alpha,1.1,(1.0-start_alpha)/len(predictions_over_time[::3]))\n",
    "for p_i,p in enumerate(predictions_over_time[::3]):\n",
    "    for xval,yval in zip(x_axis_seq,p):\n",
    "        plt.plot(xval,yval,color='r',alpha=alpha[p_i])\n",
    "\n",
    "plt.title('Evolution of Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "\n",
    "# Predicting the best test prediction you got\n",
    "plt.plot(range(df.shape[0]),all_mid_data,color='b')\n",
    "for xval,yval in zip(x_axis_seq,predictions_over_time[best_prediction_epoch]):\n",
    "    plt.plot(xval,yval,color='r')\n",
    "\n",
    "plt.title('Best Test Predictions Over Time',fontsize=18)\n",
    "plt.xlabel('Date',fontsize=18)\n",
    "plt.ylabel('Mid Price',fontsize=18)\n",
    "plt.xlim(11000,12500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"final-remarks-i'mho\">Final Remarks</h2>\n",
    "<p class=\"p-margin\">I'm hoping that you found this tutorial useful. I should mention that this was a rewarding experience for me. In this tutorial, I learned how difficult it can be to devise a model that is able to predict stock price movements correctly. You started with a motivation for why you need to model stock prices. This was followed by an explanation and code for downloading data. Then, you looked at two averaging techniques that allow you to make predictions one step into the future. You next saw that these methods are futile when you need to predict more than one step into the future. Thereafter, you discussed how you can use LSTMs to make predictions for many steps into the future. Finally, you visualized the results and saw that your model (though not perfect) is quite good at correctly predicting stock price movements.</p>\n",
    "\n",
    "<p class=\"p-margin\">Here, I'm stating several takeaways from this Notebook.</p>\n",
    "<ol>\n",
    "<li>\n",
    "<p class=\"p-margin\">Stock price/movement prediction is an extremely difficult task. Personally, I don't think any of the stock prediction models out there shouldn't be taken for granted and blindly rely on them. However, models might be able to predict stock price movement correctly most of the time, but not always.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p class=\"p-margin\">Do not be fooled by articles out there that shows predictions curves that perfectly overlaps the true stock prices. This can be replicated with a simple averaging technique and in practice it's useless. A more sensible thing to do is predicting the stock price movements.</p>\n",
    "</li>\n",
    "<li>\n",
    "<p class=\"p-margin\">The model's hyperparameters are extremely sensitive to the results you obtain. So, a very good thing to do would be to run some hyperparameter optimization techniques (for example, Grid search / Random search) on the hyperparameters. Below, I listed some of the most critical hyperparameters:</p>\n",
    "<ul>\n",
    "<li>The learning rate of the optimizer</li>\n",
    "<li>The number of layers and the number of hidden units in each layer</li>\n",
    "<li>The optimizer. I found Adam to perform the best</li>\n",
    "<li>Type of the model. You can try GRU/ Standard LSTM/ LSTM with Peepholes and evaluation performance difference</li>\n",
    "</ul>\n",
    "</li>\n",
    "<li>\n",
    "<p class=\"p-margin\">In this tutorial you did something faulty (due to the small size of data)! That is you used the test loss to decay the learning rate. This indirectly leaks information about test set into the training procedure. A better way of handling this is to have a separate validation set (apart from the test set) and decay learning rate with respect to performance of the validation set.</p>\n",
    "</li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
